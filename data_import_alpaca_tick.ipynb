{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca data tick import & processing\n",
    "*Note: I do not intend to use Alpaca data, as it only has IEX data. Also it has no delisted stocks etc. This is an exercise for myself such that I can easily manipulate data later on when I have a better data subscription. I will likely use [polygon](https://polygon.io/) for historical data and IBKR for real-time data. Even if I trade on Alpaca I will use the IBKR (5/month) live feed, because Alpaca streaming data only contains IEX, and I will not pay $99/month just to get real-time data. I hate that I do not have access to most US brokers, because all US brokers have free real-time data in contrast to Europe.*\n",
    "\n",
    "This notebook downloads tick data and converts them into 1-minute quotes.\n",
    "\n",
    "My public and private key are in ../data/alpaca/data/secret.txt\n",
    "\n",
    "First, it downloads tick data and saves them to ../data/alpaca/raw/tick/{stock}/{date}.csv\n",
    "\n",
    "Second it converts the tick data and saves the quotes to ../data/alpaca/processed/m1/quotes. The quotes contain the columns         <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. The column 'tradable' is True if the [condition](https://alpaca.markets/docs/market-data/#uqdf) 'R' (regular trading) applies. \n",
    "\n",
    "Third, it deletes the tick data.\n",
    "\n",
    "Finally, it merges the 1-minute quotes with the 1-minute OHLC to get quotebars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockQuotesRequest\n",
    "from datetime import datetime, time\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: download NBBO tick data.**\n",
    "\n",
    "Downloading SPY data for one day took 30 minutes. And this is only from the IEX exchange..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_LIST = [\"TOP\", \"APE\", \"DEM\"]\n",
    "#TOP: multi-day halts, normale halts, listed second half 2022\n",
    "#APE: very volatile, listed second half 2022\n",
    "#DEM: has dividends, lot of missing data\n",
    "START_DATE = datetime(2022, 1, 1) #datetime(2015, 12, 1) is the first available date from Alpaca\n",
    "END_DATE = datetime(2023, 7, 21)\n",
    "#END_DATE = datetime(2023, 2, 4, hour=20)  # in ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it will take too much memory or take too long, we will send a request for every day between START_DATE end END_DATE. To get all trading days between these two days, we look at the trading days from SPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2022, 1, 3) datetime.date(2022, 1, 4)\n",
      " datetime.date(2022, 1, 5) datetime.date(2022, 1, 6)\n",
      " datetime.date(2022, 1, 7)]\n",
      "[datetime.date(2023, 7, 17) datetime.date(2023, 7, 18)\n",
      " datetime.date(2023, 7, 19) datetime.date(2023, 7, 20)\n",
      " datetime.date(2023, 7, 21)]\n"
     ]
    }
   ],
   "source": [
    "SPY_df = pd.read_csv(\n",
    "        f\"../data/alpaca/raw/d1/adjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "SPY_df.set_index(SPY_df.index.tz_convert(\"US/Eastern\"), inplace=True) #Actually this isn't necessary, but we still do it for the sake of consistency\n",
    "SPY_df.set_index(SPY_df.index.tz_localize(None), inplace=True)\n",
    "dates = np.unique(pd.to_datetime(SPY_df.index).date)\n",
    "trading_dates = dates[(dates >= START_DATE.date()) & (dates <= END_DATE.date())]\n",
    "print(trading_dates[:5])\n",
    "print(trading_dates[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datetime.utcnow().replace(microsecond=0)} | Starting download\")\n",
    "with open(\"../data/alpaca/secret.txt\") as f:\n",
    "    PUBLIC_KEY = next(f).strip()\n",
    "    PRIVATE_KEY = next(f).strip()\n",
    "\n",
    "stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "\n",
    "for stock in SYMBOL_LIST:\n",
    "    os.makedirs(f\"../data/alpaca/raw/tick/{stock}\", exist_ok=True) #Create folder if it does not exist\n",
    "    for date in trading_dates:\n",
    "        start_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=4, minute=0)))\n",
    "        end_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=20, minute=0)))\n",
    "        quote_request = StockQuotesRequest(\n",
    "            symbol_or_symbols=stock, start=start_time, end=end_time\n",
    "        )\n",
    "\n",
    "        #Sometimes stock_client returns None. This can happen if the stock was not listed yet or if there are multi-day halts. In this case we should just skip the day.\n",
    "        try:\n",
    "            quotes = stock_client.get_stock_quotes(quote_request)\n",
    "        except AttributeError as e:\n",
    "            print(f\"WARNING: caught AttributeError ({e}) while downloading {stock} at {date.strftime('%Y-%m-%d')}. Likely not listed or a multi-day halt.\")\n",
    "            continue\n",
    "        \n",
    "        quotes.df.to_csv(f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\")\n",
    "        print(f\"{datetime.utcnow().replace(microsecond=0)} | Downloaded {stock} tick data for {date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 12:00:00.039820+00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['R']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 12:00:00.045994+00:00</th>\n",
       "      <td>52.51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['R']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 12:00:00.077148+00:00</th>\n",
       "      <td>52.51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['R']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 12:00:04.917189+00:00</th>\n",
       "      <td>52.51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['R']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 12:00:12.684785+00:00</th>\n",
       "      <td>48.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['R']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ask_price  ask_size  bid_price  bid_size  \\\n",
       "timestamp                                                                    \n",
       "2023-01-03 12:00:00.039820+00:00       0.00       0.0      30.69       1.0   \n",
       "2023-01-03 12:00:00.045994+00:00      52.51       3.0      30.69       1.0   \n",
       "2023-01-03 12:00:00.077148+00:00      52.51       3.0      32.25       1.0   \n",
       "2023-01-03 12:00:04.917189+00:00      52.51       3.0      35.10       1.0   \n",
       "2023-01-03 12:00:12.684785+00:00      48.35       1.0      35.10       1.0   \n",
       "\n",
       "                                 conditions  \n",
       "timestamp                                    \n",
       "2023-01-03 12:00:00.039820+00:00      ['R']  \n",
       "2023-01-03 12:00:00.045994+00:00      ['R']  \n",
       "2023-01-03 12:00:00.077148+00:00      ['R']  \n",
       "2023-01-03 12:00:04.917189+00:00      ['R']  \n",
       "2023-01-03 12:00:12.684785+00:00      ['R']  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv( \n",
    "                f\"../data/alpaca/raw/tick/DEM/2023-01-03.csv\",\n",
    "                usecols=[\n",
    "                    \"timestamp\",\n",
    "                    \"bid_size\",\n",
    "                    \"bid_price\",\n",
    "                    \"ask_price\",\n",
    "                    \"ask_size\",\n",
    "                    \"conditions\",\n",
    "                ],\n",
    "                index_col=\"timestamp\",\n",
    "                parse_dates=True,\n",
    "                nrows=5\n",
    "            )\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Retrieve NBBO data and convert to 1-minute quotes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot just use resample and then ohlc, because of halts and other special trading conditions. Also we have to think about what OHLC means and what we want. To recap, the quotes contain the columns <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. \n",
    "\n",
    "For the corresponding bid or ask in a minute timestamp at 10:00:00:\n",
    "\n",
    "* close: we want the price at 10:00:59.99999..., which is the same as 10:01:00000... The quotes at this timestamp are the last quotes. Because if no new quotes come in, the last quotes stand. If the last available tick condition is not R ('regular trading'), the quote bar becomes untradable (and \"tradable\" = <code>False</code>). For example Y means a halt.\n",
    "* open: the open has no use, it is always equal to the previous close. That is why we do not have open prices in the quotes. It is **incorrect** to use ohlc() if we do want a opening bid/ask, because the open takes the first value in the bin. Which introduces look-ahead bias. It should use the last available value.\n",
    "* high: get highest value of regular trading (excluding non-regular trading). In the backtester high & lows will only be used to accurately simulate limit and stop orders.\n",
    "* low: get lowest value of regular trading (excluding non-regular trading)\n",
    "\n",
    "*Regular trading means no halts or weird conditions. It has nothing to do with pre- or post-market.*\n",
    "* Step 1: Resample closes\n",
    "* Step 2: Create a 'tradeable' column\n",
    "* Step 3: Get the high and low bid/asks\n",
    "* Step 4: Concatenate daily DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYMBOL_LIST ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datetime.utcnow().replace(microsecond=0)} starting...\")\n",
    "for stock in SYMBOL_LIST:\n",
    "    quote_dfs = [] #A collection of dataframes for every day\n",
    "    for date in trading_dates:\n",
    "        #Need to catch if does not exist\n",
    "        try:\n",
    "            tick_day_df = pd.read_csv( \n",
    "                f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\",\n",
    "                usecols=[\n",
    "                    \"timestamp\",\n",
    "                    \"bid_size\",\n",
    "                    \"bid_price\",\n",
    "                    \"ask_price\",\n",
    "                    \"ask_size\",\n",
    "                    \"conditions\",\n",
    "                ],\n",
    "                index_col=\"timestamp\",\n",
    "                parse_dates=True,\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            # Either we didn't download it, or stock was not listed yet (e.g. APE) or there was a multi-day halt (e.g. TOP).\n",
    "            print(f\"WARNING: There is no data for {stock} at {date.strftime('%Y-%m-%d')}.\")\n",
    "            continue\n",
    "\n",
    "        tick_day_df.index.names = [\"datetime\"]\n",
    "        \n",
    "        # Convert to ET-naive as always (raw data is always in UTC-aware)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_convert(\"US/Eastern\"), inplace=True)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_localize(None), inplace=True)\n",
    "\n",
    "        #Step 1: To get the close quotes and last trade condition for m1, resample and take last value\n",
    "        minute_df = tick_day_df.resample(\"1Min\").last()\n",
    "        minute_df.rename(\n",
    "            columns={\n",
    "                \"bid_size\": \"close_bid_size\",\n",
    "                \"bid_price\": \"close_bid\",\n",
    "                \"ask_price\": \"close_ask\",\n",
    "                \"ask_size\": \"close_ask_size\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        #Due to resampling it also creates rows for weekends and non-trading hours. Hence we need to shrink it\n",
    "        all_minutes_in_day = []\n",
    "        for hour in range(4, 20):\n",
    "            for minute in range(0, 60):\n",
    "                all_minutes_in_day.append(\n",
    "                    datetime.combine(date, time(hour=hour, minute=minute))\n",
    "                )\n",
    "        all_minutes_in_day = np.array(all_minutes_in_day)\n",
    "        assert len(all_minutes_in_day) == 16 * 60\n",
    "\n",
    "        minute_df = minute_df.reindex(all_minutes_in_day)\n",
    "        \n",
    "        # Step 2: Create a 'tradeable' column which is False if there is no R in the conditions.\n",
    "        #Warning: the values in the \"conditions\" are actually strings which look like lists. \n",
    "        # So we are checking if the letter R is in the string '[R]'. \n",
    "        # We also have to check whether the value is empty (thus not a string). \n",
    "        # We could convert the values it into lists, but this works equally well.\n",
    "        tradeable = np.vectorize(lambda condition_list: \"R\" in condition_list if type(condition_list) is str else False)\n",
    "\n",
    "        minute_df.ffill(inplace=True)\n",
    "        minute_df[\"tradeable\"] = tradeable(minute_df[\"conditions\"])\n",
    "        minute_df.drop(columns=[\"conditions\"], inplace=True)\n",
    "\n",
    "        # If there were no quotes for the first minutes of the day (very unlikely), we will forward fill them with the values of the previous day. \n",
    "        # We will do that later. 'tradeable' is already false in this case because there was no 'R' in the conditions.\n",
    "\n",
    "        # Step 3: Get the high and low bid/asks. If the value is empty, it means that during the entire minute there were no new quotes. \n",
    "        # Then we must use the last quotes. Since we already filled the close_bid and close_ask with the latest quote, we can set it equal to that.\n",
    "        high_df = tick_day_df.resample(\"1Min\").max()[[\"bid_price\", \"ask_price\"]]\n",
    "        high_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"high_bid\",\n",
    "                \"ask_price\": \"high_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        low_df = tick_day_df.resample(\"1Min\").min()[[\"bid_price\", \"ask_price\"]]\n",
    "        low_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"low_bid\",\n",
    "                \"ask_price\": \"low_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=low_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=high_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "\n",
    "        # Fill high/low with close\n",
    "        minute_df[\"low_bid\"] = minute_df[\"low_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"low_ask\"] = minute_df[\"low_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "        minute_df[\"high_bid\"] = minute_df[\"high_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"high_ask\"] = minute_df[\"high_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "\n",
    "        # Reorder columns\n",
    "        minute_df = minute_df[\n",
    "                [\n",
    "                    \"close_bid_size\",\n",
    "                    \"close_bid\",\n",
    "                    \"close_ask\",\n",
    "                    \"close_ask_size\",\n",
    "                    \"high_bid\",\n",
    "                    \"high_ask\",\n",
    "                    \"low_bid\",\n",
    "                    \"low_ask\",\n",
    "                    \"tradeable\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        # Append the df (which only contains the minutes of 1 day) to quote_dfs.\n",
    "        quote_dfs.append(minute_df) #quote_dfs[0] contains the DataFrame from the first day; quote_dfs[1] the second day etc.\n",
    "\n",
    "        print(f\"Processed {stock} tick data for {date.strftime('%Y-%m-%d')}\")\n",
    "    # Step 4: Concatenate daily DataFrames\n",
    "    quote_df = pd.concat(quote_dfs)\n",
    "\n",
    "    # As said in step 2, if there were no quotes for the first minutes of the day (very unlikely), \n",
    "    # we will forward fill them with the values of the previous day. 'tradeable' is already False. \n",
    "    if quote_df.isnull().any().any():\n",
    "        amount_null = quote_df.isna().sum().sum()\n",
    "        print(\n",
    "            f\"{stock} | WARNING: dataframe contain {amount_null} null values, which will be forward filled.\"\n",
    "        )\n",
    "        quote_df = quote_df.ffill().bfill() #Very rarely it can happen that at the start there are no quotes. That is why we also need to backfill. \n",
    "\n",
    "    quote_df.to_csv(f\"../data/alpaca/processed/m1/quotes/{stock}.csv\")\n",
    "    print(f\"{datetime.utcnow().replace(microsecond=0)} | {stock} | Processed tick data to quotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Combine quotes and bars to get quotebars.**\n",
    "\n",
    "* Step 1: Left merge quotes to bars.\n",
    "* Step 2: Adjust quotes using the adjustment factor and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYMBOL_LIST = [\"TOP\", \"APE\", \"DEM\"]\n",
    "# START_DATE = datetime(2022, 1, 1) #datetime(2015, 12, 1) is the first available date from Alpaca\n",
    "# END_DATE = datetime(2023, 7, 21)\n",
    "MARKET_HOURS_ONLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stock in SYMBOL_LIST:\n",
    "for stock in SYMBOL_LIST:\n",
    "    quotes = pd.read_csv(\n",
    "        f\"../data/alpaca/processed/m1/quotes/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    bars = pd.read_csv(\n",
    "        f\"../data/alpaca/processed/m1/bars/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    quotes = quotes[\n",
    "            (quotes.index >= START_DATE.replace(hour=4, minute=0, second=0))\n",
    "            & (quotes.index <= END_DATE.replace(hour=19, minute=59, second=0))\n",
    "        ]\n",
    "    bars = bars[\n",
    "            (bars.index >= START_DATE.replace(hour=4, minute=0, second=0))\n",
    "            & (bars.index <= END_DATE.replace(hour=19, minute=59, second=0))\n",
    "        ]\n",
    "    if MARKET_HOURS_ONLY == True:\n",
    "        quotes = quotes.between_time(\"9:30\", \"15:59\")\n",
    "        bars = bars.between_time(\"9:30\", \"15:59\")\n",
    "\n",
    "    # The \"tradable\" from the quotes is leading. The \"tradable\" from quotes was just approximated by setting \"tradable\" \n",
    "    # to True if the data was empty. The \"tradable\" from quotes are whether the stock was actually tradable.\n",
    "    quotebars = pd.merge(\n",
    "        left=bars[[\"open\", \"high\", \"low\", \"close\", \"close_original\", \"volume\", \"adjustment\"]],\n",
    "        right=quotes[[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    quotebars = quotebars[[\"open\", \"high\", \"low\", \"close\", \"close_original\", \"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"volume\", \"tradeable\", \"adjustment\"]]\n",
    "\n",
    "    if len(bars) != len(quotes):\n",
    "        print(f\"{stock} | There is a difference between the amount of bars and quotes.\")\n",
    "        print(f\"There are {len(bars) - len(quotes)} less quotes than bars\")\n",
    "        # If there are less quotes than bars, tradable will be set to 'False'. The values of the quotes don't \n",
    "        # matter at this point (because it will never be used if 'tradable' is False). Hence we will just fill \n",
    "        # it with zeroes. This is only the case with multi-day halts.\n",
    "        quotebars['tradeable'] = quotebars['tradeable'].fillna(False)\n",
    "        quotebars[[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\"]] = quotebars[[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\"]].fillna(0.0)\n",
    " \n",
    "    # The quotes are NOT adjusted for dividends and splits. So we need to adjust them such \n",
    "    # that ALL values in bars and quotebars are adjusted.\n",
    "    quotebars[[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\"]] = quotebars[[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\"]].multiply(quotebars[\"adjustment\"], axis=\"index\").round(decimals=5) #Rounding to save space, but may cause small precision errors\n",
    "\n",
    "    quotebars.to_csv(f\"../data/alpaca/processed/m1/quotebars/{stock}.csv\")\n",
    "    print(f'Processed {stock} to quotebars')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>close_original</th>\n",
       "      <th>close_bid_size</th>\n",
       "      <th>close_bid</th>\n",
       "      <th>close_ask</th>\n",
       "      <th>close_ask_size</th>\n",
       "      <th>high_bid</th>\n",
       "      <th>high_ask</th>\n",
       "      <th>low_bid</th>\n",
       "      <th>low_ask</th>\n",
       "      <th>volume</th>\n",
       "      <th>tradeable</th>\n",
       "      <th>adjustment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03 09:30:00</th>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>43.372093</td>\n",
       "      <td>26.95512</td>\n",
       "      <td>38.92319</td>\n",
       "      <td>39.07594</td>\n",
       "      <td>26.95512</td>\n",
       "      <td>38.94116</td>\n",
       "      <td>39.12985</td>\n",
       "      <td>38.86030</td>\n",
       "      <td>39.06696</td>\n",
       "      <td>2634.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03 09:31:00</th>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>43.372093</td>\n",
       "      <td>0.89850</td>\n",
       "      <td>38.86030</td>\n",
       "      <td>39.17478</td>\n",
       "      <td>8.98504</td>\n",
       "      <td>38.92319</td>\n",
       "      <td>39.17478</td>\n",
       "      <td>38.86030</td>\n",
       "      <td>39.05797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03 09:32:00</th>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>38.97</td>\n",
       "      <td>43.372093</td>\n",
       "      <td>26.05662</td>\n",
       "      <td>38.88725</td>\n",
       "      <td>39.04000</td>\n",
       "      <td>26.05662</td>\n",
       "      <td>38.89624</td>\n",
       "      <td>39.17478</td>\n",
       "      <td>38.86030</td>\n",
       "      <td>39.02203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03 09:33:00</th>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>43.416611</td>\n",
       "      <td>27.85362</td>\n",
       "      <td>38.88725</td>\n",
       "      <td>39.04000</td>\n",
       "      <td>29.65063</td>\n",
       "      <td>38.90522</td>\n",
       "      <td>39.04899</td>\n",
       "      <td>38.86030</td>\n",
       "      <td>39.04000</td>\n",
       "      <td>369.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03 09:34:00</th>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>39.01</td>\n",
       "      <td>43.416611</td>\n",
       "      <td>26.95512</td>\n",
       "      <td>38.89624</td>\n",
       "      <td>39.02203</td>\n",
       "      <td>5.39102</td>\n",
       "      <td>38.91421</td>\n",
       "      <td>39.04000</td>\n",
       "      <td>38.87827</td>\n",
       "      <td>39.02203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      open   high    low  close  close_original  \\\n",
       "datetime                                                          \n",
       "2022-01-03 09:30:00  38.97  38.97  38.97  38.97       43.372093   \n",
       "2022-01-03 09:31:00  38.97  38.97  38.97  38.97       43.372093   \n",
       "2022-01-03 09:32:00  38.97  38.97  38.97  38.97       43.372093   \n",
       "2022-01-03 09:33:00  39.01  39.01  39.01  39.01       43.416611   \n",
       "2022-01-03 09:34:00  39.01  39.01  39.01  39.01       43.416611   \n",
       "\n",
       "                     close_bid_size  close_bid  close_ask  close_ask_size  \\\n",
       "datetime                                                                    \n",
       "2022-01-03 09:30:00        26.95512   38.92319   39.07594        26.95512   \n",
       "2022-01-03 09:31:00         0.89850   38.86030   39.17478         8.98504   \n",
       "2022-01-03 09:32:00        26.05662   38.88725   39.04000        26.05662   \n",
       "2022-01-03 09:33:00        27.85362   38.88725   39.04000        29.65063   \n",
       "2022-01-03 09:34:00        26.95512   38.89624   39.02203         5.39102   \n",
       "\n",
       "                     high_bid  high_ask   low_bid   low_ask  volume  \\\n",
       "datetime                                                              \n",
       "2022-01-03 09:30:00  38.94116  39.12985  38.86030  39.06696  2634.0   \n",
       "2022-01-03 09:31:00  38.92319  39.17478  38.86030  39.05797     0.0   \n",
       "2022-01-03 09:32:00  38.89624  39.17478  38.86030  39.02203     0.0   \n",
       "2022-01-03 09:33:00  38.90522  39.04899  38.86030  39.04000   369.0   \n",
       "2022-01-03 09:34:00  38.91421  39.04000  38.87827  39.02203     0.0   \n",
       "\n",
       "                     tradeable  adjustment  \n",
       "datetime                                    \n",
       "2022-01-03 09:30:00       True    0.898504  \n",
       "2022-01-03 09:31:00       True    0.898504  \n",
       "2022-01-03 09:32:00       True    0.898504  \n",
       "2022-01-03 09:33:00       True    0.898504  \n",
       "2022-01-03 09:34:00       True    0.898504  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv( \n",
    "                f\"../data/alpaca/processed/m1/quotebars/DEM.csv\",\n",
    "                index_col=\"datetime\",\n",
    "                parse_dates=True,\n",
    "                nrows=5\n",
    "            )\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Check for weird quotes**\n",
    "\n",
    "If the quotebar is tradeable:\n",
    "* The OHLC should be within or equal to the quotes\n",
    "* The quotes should not deviate too much from OHLC. I choose a maximum acceptable deviation of 5%.\n",
    "\n",
    "Since we only have IEX data and I deliberately choose less liquid stocks to decrease downloading time, there will be a lot of weird quotes. Because I won't be using this for real trading, I will not 'clean' this data. If we were to have NBBO data from all exchanges, the amount of weird quotes would go down significantly. I probably wouldn't even have to clean it anymore. Besides, the backtester should handle situations in which the asset is tradeable but has an unacceptably high spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYMBOL_LIST = [\"TOP\", \"APE\", \"DEM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP | TRADES NOT CONTAINED IN QUOTES\n",
      "    close: 1.56% is not correct\n",
      "    high: 0.19% is not correct\n",
      "    low: 0.20% is not correct\n",
      "TOP | QUOTES DEVIATING >5% FROM TRADES\n",
      "    close: 0.00% is not correct\n",
      "    high: 0.44% is not correct\n",
      "    low: 0.28% is not correct\n",
      "=============================================================\n",
      "APE | TRADES NOT CONTAINED IN QUOTES\n",
      "    close: 0.02% is not correct\n",
      "    high: 0.05% is not correct\n",
      "    low: 0.04% is not correct\n",
      "APE | QUOTES DEVIATING >5% FROM TRADES\n",
      "    close: 0.00% is not correct\n",
      "    high: 0.03% is not correct\n",
      "    low: 0.03% is not correct\n",
      "=============================================================\n",
      "DEM | TRADES NOT CONTAINED IN QUOTES\n",
      "    close: 2.64% is not correct\n",
      "    high: 0.03% is not correct\n",
      "    low: 0.03% is not correct\n",
      "DEM | QUOTES DEVIATING >5% FROM TRADES\n",
      "    close: 0.00% is not correct\n",
      "    high: 0.00% is not correct\n",
      "    low: 0.00% is not correct\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "for stock in SYMBOL_LIST:\n",
    "    quotebars = pd.read_csv(\n",
    "            f\"../data/alpaca/processed/m1/quotebars/{stock}.csv\",\n",
    "            index_col=\"datetime\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "    # ['open', 'high', 'low', 'close', 'close_original', 'close_bid_size',\n",
    "    #        'close_bid', 'close_ask', 'close_ask_size', 'high_bid', 'high_ask',\n",
    "    #        'low_bid', 'low_ask', 'volume', 'tradeable', 'adjustment'],\n",
    "\n",
    "    quotebars = quotebars[quotebars['tradeable']==True]\n",
    "    rows = len(quotebars)\n",
    "    # If there were no trades, close is forward filled in the OHLC bars\n",
    "    # It can then easily happen that is it not contained between bid/ask\n",
    "    # Because bid/ask can change without there being a trade\n",
    "    quotebars = quotebars[quotebars['volume'] > 0]\n",
    "\n",
    "    # 1. Check percentage of quotebars where OHLC is outside quotes\n",
    "    # The extra 0.01 tolerance if for rounding issues due to adjustments\n",
    "    quotebars = quotebars[['open', 'high', 'low', 'close', 'close_bid_size','close_bid', 'close_ask', 'close_ask_size','high_bid', 'high_ask','low_bid', 'low_ask', 'volume']]\n",
    "    not_contained_close = quotebars[(quotebars['close'] < quotebars['close_bid']-0.01) | ((quotebars['close'] > quotebars['close_ask']+0.01))]\n",
    "    not_contained_high = quotebars[(quotebars['high'] > quotebars['high_ask']+0.01)]\n",
    "    not_contained_low = quotebars[(quotebars['low'] < quotebars['low_bid']-0.01)]\n",
    "\n",
    "    not_contained_close = not_contained_close[['close', 'close_bid', 'close_ask', 'volume']]\n",
    "    not_contained_high = not_contained_high[['high', 'high_ask', 'volume']]\n",
    "    not_contained_low = not_contained_low[['low', 'low_bid', 'volume']]\n",
    "\n",
    "    print(f\"{stock} | TRADES NOT CONTAINED IN QUOTES\")\n",
    "    print(f\"    close: {100*(len(not_contained_close)/rows):.2f}% is not correct\")\n",
    "    print(f\"    high: {100*(len(not_contained_high)/rows):.2f}% is not correct\")\n",
    "    print(f\"    low: {100*(len(not_contained_low)/rows):.2f}% is not correct\")\n",
    "\n",
    "    # 2. Check if quotes are not too far from OHLC\n",
    "    too_far_close = quotebars[(quotebars['close']*1.05 < quotebars['close_bid']) | ((quotebars['close']*0.95 > quotebars['close_ask']))]\n",
    "    too_far_high = quotebars[(quotebars['high_ask'] > quotebars['high']*1.05)]\n",
    "    too_far_low = quotebars[(quotebars['low_bid'] < quotebars['low']*0.95)]\n",
    "\n",
    "    too_far_close = too_far_close[['close', 'close_bid', 'close_ask', 'volume']]\n",
    "    too_far_high = too_far_high[['high', 'high_ask', 'volume']]\n",
    "    too_far_low = too_far_low[['low', 'low_bid', 'volume']]\n",
    "\n",
    "    print(f\"{stock} | QUOTES DEVIATING >5% FROM TRADES\")\n",
    "    print(f\"    close: {100*(len(too_far_close)/rows):.2f}% is not correct\")\n",
    "    print(f\"    high: {100*(len(too_far_high)/rows):.2f}% is not correct\")\n",
    "    print(f\"    low: {100*(len(too_far_low)/rows):.2f}% is not correct\")\n",
    "    print(f\"=============================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Delete all tick data**\n",
    "\n",
    "We do not need the tick data anymore and it takes up an enormous amount of disk space. For reference, the ['Complete Tick Data'](https://firstratedata.com/tick-data) bundle at [firstratedata.com](https://firstratedata.com/) which contains trades and quotes of 5000 stocks & ETFs for almost 14 years is 24 TB. I only have 2 disks of 0.25TB... I ordered a 4TB disk, I hope that is more than enough.\n",
    "\n",
    "*I eventually decided to just delete manually because that is easier.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
