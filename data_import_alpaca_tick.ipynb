{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca data import & processing\n",
    "This notebook downloads tick data and converts them into 1-minute quotes.\n",
    "\n",
    "My public and private key are in ../data/alpaca/data/secret.txt\n",
    "\n",
    "First, it downloads tick data and saves them to ../data/alpaca/raw/tick/{stock}/{date}.csv\n",
    "\n",
    "Second it converts the tick data and saves the quotes to ../data/alpaca/processed/m1/quotes. The quotes contain the columns         <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. The column 'tradable' is True if the [condition](https://alpaca.markets/docs/market-data/#uqdf) 'R' (regular trading) applies. \n",
    "\n",
    "Third, it deletes the tick data.\n",
    "\n",
    "Finally, it merges the 1-minute quotes with the 1-minute OHLC to get quotebars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockQuotesRequest\n",
    "from datetime import datetime, time\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: download NBBO tick data.**\n",
    "\n",
    "Downloading SPY data for one day took 30 minutes. And this is only from the IEX exchange..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_LIST = [\"TOP\", \"APE\"]\n",
    "START_DATE = datetime(2022, 5, 12) #datetime(2015, 12, 1) is the first available date from Alpaca\n",
    "END_DATE = datetime(2023, 7, 21)\n",
    "#END_DATE = datetime(2023, 2, 4, hour=20)  # in ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it will take too much memory or take too long, we will send a request for every day between START_DATE end END_DATE. To get all trading days between these two days, we look at the trading days from SPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2022, 12, 1), datetime.date(2022, 12, 2),\n",
       "       datetime.date(2022, 12, 5), datetime.date(2022, 12, 6),\n",
       "       datetime.date(2022, 12, 7), datetime.date(2022, 12, 8),\n",
       "       datetime.date(2022, 12, 9), datetime.date(2022, 12, 12),\n",
       "       datetime.date(2022, 12, 13), datetime.date(2022, 12, 14),\n",
       "       datetime.date(2022, 12, 15), datetime.date(2022, 12, 16),\n",
       "       datetime.date(2022, 12, 19), datetime.date(2022, 12, 20),\n",
       "       datetime.date(2022, 12, 21), datetime.date(2022, 12, 22),\n",
       "       datetime.date(2022, 12, 23), datetime.date(2022, 12, 27),\n",
       "       datetime.date(2022, 12, 28), datetime.date(2022, 12, 29),\n",
       "       datetime.date(2022, 12, 30), datetime.date(2023, 1, 3),\n",
       "       datetime.date(2023, 1, 4), datetime.date(2023, 1, 5),\n",
       "       datetime.date(2023, 1, 6), datetime.date(2023, 1, 9),\n",
       "       datetime.date(2023, 1, 10), datetime.date(2023, 1, 11),\n",
       "       datetime.date(2023, 1, 12), datetime.date(2023, 1, 13),\n",
       "       datetime.date(2023, 1, 17), datetime.date(2023, 1, 18),\n",
       "       datetime.date(2023, 1, 19), datetime.date(2023, 1, 20),\n",
       "       datetime.date(2023, 1, 23), datetime.date(2023, 1, 24),\n",
       "       datetime.date(2023, 1, 25), datetime.date(2023, 1, 26),\n",
       "       datetime.date(2023, 1, 27), datetime.date(2023, 1, 30),\n",
       "       datetime.date(2023, 1, 31), datetime.date(2023, 2, 1),\n",
       "       datetime.date(2023, 2, 2), datetime.date(2023, 2, 3),\n",
       "       datetime.date(2023, 2, 6), datetime.date(2023, 2, 7),\n",
       "       datetime.date(2023, 2, 8), datetime.date(2023, 2, 9),\n",
       "       datetime.date(2023, 2, 10), datetime.date(2023, 2, 13),\n",
       "       datetime.date(2023, 2, 14), datetime.date(2023, 2, 15),\n",
       "       datetime.date(2023, 2, 16), datetime.date(2023, 2, 17),\n",
       "       datetime.date(2023, 2, 21), datetime.date(2023, 2, 22),\n",
       "       datetime.date(2023, 2, 23), datetime.date(2023, 2, 24),\n",
       "       datetime.date(2023, 2, 27), datetime.date(2023, 2, 28),\n",
       "       datetime.date(2023, 3, 1), datetime.date(2023, 3, 2),\n",
       "       datetime.date(2023, 3, 3), datetime.date(2023, 3, 6),\n",
       "       datetime.date(2023, 3, 7), datetime.date(2023, 3, 8),\n",
       "       datetime.date(2023, 3, 9), datetime.date(2023, 3, 10),\n",
       "       datetime.date(2023, 3, 13), datetime.date(2023, 3, 14),\n",
       "       datetime.date(2023, 3, 15), datetime.date(2023, 3, 16),\n",
       "       datetime.date(2023, 3, 17), datetime.date(2023, 3, 20),\n",
       "       datetime.date(2023, 3, 21), datetime.date(2023, 3, 22),\n",
       "       datetime.date(2023, 3, 23), datetime.date(2023, 3, 24),\n",
       "       datetime.date(2023, 3, 27), datetime.date(2023, 3, 28),\n",
       "       datetime.date(2023, 3, 29), datetime.date(2023, 3, 30),\n",
       "       datetime.date(2023, 3, 31), datetime.date(2023, 4, 3),\n",
       "       datetime.date(2023, 4, 4), datetime.date(2023, 4, 5),\n",
       "       datetime.date(2023, 4, 6), datetime.date(2023, 4, 10),\n",
       "       datetime.date(2023, 4, 11), datetime.date(2023, 4, 12),\n",
       "       datetime.date(2023, 4, 13), datetime.date(2023, 4, 14),\n",
       "       datetime.date(2023, 4, 17), datetime.date(2023, 4, 18),\n",
       "       datetime.date(2023, 4, 19), datetime.date(2023, 4, 20),\n",
       "       datetime.date(2023, 4, 21), datetime.date(2023, 4, 24),\n",
       "       datetime.date(2023, 4, 25), datetime.date(2023, 4, 26),\n",
       "       datetime.date(2023, 4, 27), datetime.date(2023, 4, 28),\n",
       "       datetime.date(2023, 5, 1), datetime.date(2023, 5, 2),\n",
       "       datetime.date(2023, 5, 3), datetime.date(2023, 5, 4),\n",
       "       datetime.date(2023, 5, 5), datetime.date(2023, 5, 8),\n",
       "       datetime.date(2023, 5, 9), datetime.date(2023, 5, 10),\n",
       "       datetime.date(2023, 5, 11), datetime.date(2023, 5, 12),\n",
       "       datetime.date(2023, 5, 15), datetime.date(2023, 5, 16),\n",
       "       datetime.date(2023, 5, 17), datetime.date(2023, 5, 18),\n",
       "       datetime.date(2023, 5, 19), datetime.date(2023, 5, 22),\n",
       "       datetime.date(2023, 5, 23), datetime.date(2023, 5, 24),\n",
       "       datetime.date(2023, 5, 25), datetime.date(2023, 5, 26),\n",
       "       datetime.date(2023, 5, 30), datetime.date(2023, 5, 31),\n",
       "       datetime.date(2023, 6, 1), datetime.date(2023, 6, 2),\n",
       "       datetime.date(2023, 6, 5), datetime.date(2023, 6, 6),\n",
       "       datetime.date(2023, 6, 7), datetime.date(2023, 6, 8),\n",
       "       datetime.date(2023, 6, 9), datetime.date(2023, 6, 12),\n",
       "       datetime.date(2023, 6, 13), datetime.date(2023, 6, 14),\n",
       "       datetime.date(2023, 6, 15), datetime.date(2023, 6, 16),\n",
       "       datetime.date(2023, 6, 20), datetime.date(2023, 6, 21),\n",
       "       datetime.date(2023, 6, 22), datetime.date(2023, 6, 23),\n",
       "       datetime.date(2023, 6, 26), datetime.date(2023, 6, 27),\n",
       "       datetime.date(2023, 6, 28), datetime.date(2023, 6, 29),\n",
       "       datetime.date(2023, 6, 30), datetime.date(2023, 7, 3),\n",
       "       datetime.date(2023, 7, 5), datetime.date(2023, 7, 6),\n",
       "       datetime.date(2023, 7, 7), datetime.date(2023, 7, 10),\n",
       "       datetime.date(2023, 7, 11), datetime.date(2023, 7, 12),\n",
       "       datetime.date(2023, 7, 13), datetime.date(2023, 7, 14),\n",
       "       datetime.date(2023, 7, 17), datetime.date(2023, 7, 18),\n",
       "       datetime.date(2023, 7, 19), datetime.date(2023, 7, 20),\n",
       "       datetime.date(2023, 7, 21)], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPY_df = pd.read_csv(\n",
    "        f\"../data/alpaca/raw/d1/adjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "SPY_df.set_index(SPY_df.index.tz_convert(\"US/Eastern\"), inplace=True) #Actually this isn't necessary, but we still do it for the sake of consistency\n",
    "SPY_df.set_index(SPY_df.index.tz_localize(None), inplace=True)\n",
    "dates = np.unique(pd.to_datetime(SPY_df.index).date)\n",
    "trading_dates = dates[(dates >= START_DATE.date()) & (dates <= END_DATE.date())]\n",
    "trading_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-29 15:32:21 | Starting download\n",
      "2023-07-29 15:32:22.753506 | Downloaded TOP tick data for 2022-12-01\n",
      "2023-07-29 15:32:23.754719 | Downloaded TOP tick data for 2022-12-02\n",
      "2023-07-29 15:32:24.511194 | Downloaded TOP tick data for 2022-12-05\n",
      "2023-07-29 15:32:25.253937 | Downloaded TOP tick data for 2022-12-06\n",
      "2023-07-29 15:32:25.773445 | Downloaded TOP tick data for 2022-12-07\n",
      "2023-07-29 15:32:26.230227 | Downloaded TOP tick data for 2022-12-08\n",
      "2023-07-29 15:32:26.476562 | Downloaded TOP tick data for 2022-12-09\n",
      "2023-07-29 15:32:26.694156 | Downloaded TOP tick data for 2022-12-12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWARNING: caught Attribute Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m while downloading \u001b[39m\u001b[39m{\u001b[39;00mstock\u001b[39m}\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m{\u001b[39;00mdate\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m. Likely a multi-day halt.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m quotes\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/alpaca/raw/tick/\u001b[39m\u001b[39m{\u001b[39;00mstock\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdate\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mutcnow()\u001b[39m}\u001b[39;00m\u001b[39m | Downloaded \u001b[39m\u001b[39m{\u001b[39;00mstock\u001b[39m}\u001b[39;00m\u001b[39m tick data for \u001b[39m\u001b[39m{\u001b[39;00mdate\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\alpaca\\data\\models\\base.py:21\u001b[0m, in \u001b[0;36mTimeSeriesMixin.df\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a pandas dataframe containing the bar data.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mRequires mapping to be defined in child class.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m    DataFrame: data in a pandas dataframe\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# combine all lists of data into one list\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m data_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdict()\u001b[39m.\u001b[39mvalues()))\n\u001b[0;32m     23\u001b[0m \u001b[39m# set multi-level index\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# level=0 - symbol\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# level=1 - timestamp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data_list)\u001b[39m.\u001b[39mset_index([\u001b[39m\"\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\alpaca\\data\\models\\base.py:60\u001b[0m, in \u001b[0;36mBaseDataSet.dict\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mGives dictionary representation of data.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m    dict: The data in dictionary form.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# converts each data (Bar, Quote, etc) in the symbol specific lists to its dictionary format\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     61\u001b[0m     symbol: \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m d: d\u001b[39m.\u001b[39;49mdict(), data_list))\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;49;00m symbol, data_list \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mitems()\n\u001b[0;32m     63\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\alpaca\\data\\models\\base.py:61\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mGives dictionary representation of data.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m    dict: The data in dictionary form.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# converts each data (Bar, Quote, etc) in the symbol specific lists to its dictionary format\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m---> 61\u001b[0m     symbol: \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m d: d\u001b[39m.\u001b[39mdict(), data_list))\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m symbol, data_list \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\alpaca\\data\\models\\base.py:61\u001b[0m, in \u001b[0;36mBaseDataSet.dict.<locals>.<lambda>\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mGives dictionary representation of data.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m    dict: The data in dictionary form.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# converts each data (Bar, Quote, etc) in the symbol specific lists to its dictionary format\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m---> 61\u001b[0m     symbol: \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m d: d\u001b[39m.\u001b[39;49mdict(), data_list))\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m symbol, data_list \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pydantic\\main.py:449\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pydantic\\main.py:868\u001b[0m, in \u001b[0;36m_iter\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pydantic\\main.py:794\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel._get_value\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pydantic\\typing.py:438\u001b[0m, in \u001b[0;36mpydantic.typing.is_namedtuple\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:409\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"{datetime.utcnow().replace(microsecond=0)} | Starting download\")\n",
    "with open(\"../data/alpaca/secret.txt\") as f:\n",
    "    PUBLIC_KEY = next(f).strip()\n",
    "    PRIVATE_KEY = next(f).strip()\n",
    "\n",
    "stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "\n",
    "for stock in SYMBOL_LIST:\n",
    "    os.makedirs(f\"../data/alpaca/raw/tick/{stock}\", exist_ok=True) #Create folder if it does not exist\n",
    "    for date in trading_dates:\n",
    "        start_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=4, minute=0)))\n",
    "        end_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=20, minute=0)))\n",
    "        quote_request = StockQuotesRequest(\n",
    "            symbol_or_symbols=stock, start=start_time, end=end_time\n",
    "        )\n",
    "\n",
    "        #In the rare case that there are multi-day halts, the stock_client returns None. In this case we should just skip the day.\n",
    "        try:\n",
    "            quotes = stock_client.get_stock_quotes(quote_request)\n",
    "        except AttributeError as e:\n",
    "            print(f\"WARNING: caught {e} while downloading {stock} at {date.strftime('%Y-%m-%d')}. Likely a multi-day halt.\")\n",
    "            continue\n",
    "        \n",
    "        quotes.df.to_csv(f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\")\n",
    "        print(f\"{datetime.utcnow()} | Downloaded {stock} tick data for {date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Get tick data and convert to quotes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot just use resample and then ohlc, because of halts and other special trading conditions. Also we have to think about what OHLC means and what we want. To recap, the quotes contain the columns <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. \n",
    "\n",
    "For the corresponding bid or ask in a minute timestamp at 10:00:00:\n",
    "\n",
    "* close: we want the price at 10:00:59.99999..., which is the same as 10:01:00000... The quotes at this timestamp are the last quotes. Because if no new quotes come in, the last quotes stand. If the last available tick condition is not R ('regular trading'), the quote bar becomes untradable (and \"tradable\" = <code>False</code>). For example Y means a halt.\n",
    "* open: the open has no use, it is always equal to the previous close. That is why we do not have open prices in the quotes. It is **incorrect** to use ohlc() if we do want a opening bid/ask, because the open takes the first value in the bin. Which introduces look-ahead bias. It should use the last available value.\n",
    "* high: get highest value of regular trading (excluding non-regular trading)\n",
    "* low: get lowest value of regular trading (excluding non-regular trading)\n",
    "\n",
    "*Regular trading means no halts or weird conditions. It has nothing to do with pre- or post-market.*\n",
    "* Step 1: Resample closes\n",
    "* Step 2: Create a 'tradeable' column\n",
    "* Step 3: Get the high and low bid/asks\n",
    "* Step 4: Concatenate daily DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_LIST = [\"TOP\", \"AMC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in SYMBOL_LIST:\n",
    "    quote_dfs = [] #A collection of dataframes for every day\n",
    "    for date in trading_dates:\n",
    "        tick_day_df = pd.read_csv(\n",
    "            f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\",\n",
    "            usecols=[\n",
    "                \"timestamp\",\n",
    "                \"bid_size\",\n",
    "                \"bid_price\",\n",
    "                \"ask_price\",\n",
    "                \"ask_size\",\n",
    "                \"conditions\",\n",
    "            ],\n",
    "            index_col=\"timestamp\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        tick_day_df.index.names = [\"datetime\"]\n",
    "        \n",
    "        # Convert to ET-naive as always (raw data is always in UTC-aware)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_convert(\"US/Eastern\"), inplace=True)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_localize(None), inplace=True)\n",
    "\n",
    "        #Step 1: To get the close quotes and last trade condition for m1, resample and take last value\n",
    "        minute_df = tick_day_df.resample(\"1Min\").last()\n",
    "        minute_df.rename(\n",
    "            columns={\n",
    "                \"bid_size\": \"close_bid_size\",\n",
    "                \"bid_price\": \"close_bid\",\n",
    "                \"ask_price\": \"close_ask\",\n",
    "                \"ask_size\": \"close_ask_size\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        #Due to resampling it also creates rows for weekends and non-trading hours. Hence we need to shrink it\n",
    "        all_minutes_in_day = []\n",
    "        for hour in range(4, 20):\n",
    "            for minute in range(0, 60):\n",
    "                all_minutes_in_day.append(\n",
    "                    datetime.combine(date, time(hour=hour, minute=minute))\n",
    "                )\n",
    "        all_minutes_in_day = np.array(all_minutes_in_day)\n",
    "        assert len(all_minutes_in_day) == 16 * 60\n",
    "\n",
    "        minute_df = minute_df.reindex(all_minutes_in_day)\n",
    "\n",
    "        #Step 2: Create a 'tradeable' column which is False if there is no R in the conditions.\n",
    "        tradeable = np.vectorize(lambda condition_list: \"R\" in condition_list if condition_list is not None else False)\n",
    "\n",
    "        minute_df.ffill(inplace=True)\n",
    "        minute_df[\"tradeable\"] = tradeable(minute_df[\"conditions\"])\n",
    "        minute_df.drop(columns=[\"conditions\"], inplace=True)\n",
    "\n",
    "        #If there were no quotes for the first minutes of the day (very unlikely), we will forward fill them with the values of the previous day. We will do that later. 'tradeable' is already false in this case because there was no 'R' in the conditions.\n",
    "\n",
    "        #Step 3: Get the high and low bid/asks. If the value is empty, it means that during the entire minute there were no new quotes. Then we must use the last quotes. Since we already filled the close_bid and close_ask with the latest quote, we can set it equal to that.\n",
    "        high_df = tick_day_df.resample(\"1Min\").max()[[\"bid_price\", \"ask_price\"]]\n",
    "        high_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"high_bid\",\n",
    "                \"ask_price\": \"high_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        low_df = tick_day_df.resample(\"1Min\").min()[[\"bid_price\", \"ask_price\"]]\n",
    "        low_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"low_bid\",\n",
    "                \"ask_price\": \"low_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=low_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=high_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "\n",
    "        # Fill high/low with close\n",
    "        minute_df[\"low_bid\"] = minute_df[\"low_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"low_ask\"] = minute_df[\"low_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "        minute_df[\"high_bid\"] = minute_df[\"high_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"high_ask\"] = minute_df[\"high_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "\n",
    "        # Reorder columns\n",
    "        minute_df = minute_df[\n",
    "                [\n",
    "                    \"close_bid_size\",\n",
    "                    \"close_bid\",\n",
    "                    \"close_ask\",\n",
    "                    \"close_ask_size\",\n",
    "                    \"high_bid\",\n",
    "                    \"high_ask\",\n",
    "                    \"low_bid\",\n",
    "                    \"low_ask\",\n",
    "                    \"tradeable\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        # Append the df (which only contains the minutes of 1 day) to quote_dfs.\n",
    "        quote_dfs.append(minute_df) #quote_dfs[0] contains the DataFrame from the first day; quote_dfs[1] the second day etc.\n",
    "\n",
    "    # Step 4: Concatenate daily DataFrames\n",
    "    quote_df = pd.concat(quote_dfs)\n",
    "\n",
    "    # As said in step 2, if there were no quotes for the first minutes of the day (very unlikely), we will forward fill them with the values of the previous day. 'tradeable' is already False. \n",
    "    if quote_df.isnull().any().any():\n",
    "        amount_null = quote_df.isna().sum().sum()\n",
    "        print(\n",
    "            f\"{stock} | WARNING: dataframe contain {amount_null} null values, which will be forward filled.\"\n",
    "        )\n",
    "        quote_df.ffill(inplace=True)\n",
    "\n",
    "    quote_df.to_csv(f\"../data/alpaca/processed/m1/quotes/{stock}.csv\")\n",
    "    print(f\"{datetime.utcnow().replace(microsecond=0)} | {stock} | Processed tick data to quotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Delete all tick data**\n",
    "\n",
    "We do not need the tick data anymore and it takes up an enormous amount of disk space. For reference, the ['Complete Tick Data'](https://firstratedata.com/tick-data) bundle at [firstratedata.com](https://firstratedata.com/) which contains trades and quotes of 5000 stocks & ETFs for almost 14 years is 24 TB. I only have 2 disks of 0.25TB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
