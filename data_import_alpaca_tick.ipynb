{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca data import & processing\n",
    "This notebook downloads tick data and converts them into 1-minute quotes.\n",
    "\n",
    "My public and private key are in ../data/alpaca/data/secret.txt\n",
    "\n",
    "First, it downloads tick data and saves them to ../data/alpaca/raw/tick/{stock}/{date}.csv\n",
    "\n",
    "Second it converts the tick data and saves the quotes to ../data/alpaca/processed/m1/quotes. The quotes contain the columns         <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. The column 'tradable' is True if the [condition](https://alpaca.markets/docs/market-data/#uqdf) 'R' (regular trading) applies. \n",
    "\n",
    "Third, it deletes the tick data.\n",
    "\n",
    "Finally, it merges the 1-minute quotes with the 1-minute OHLC to get quotebars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockQuotesRequest\n",
    "from datetime import datetime, time\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: download NBBO tick data.**\n",
    "\n",
    "Downloading SPY data for one day took 30 minutes. And this is only from the IEX exchange..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_LIST = [\"TOP\"]\n",
    "START_DATE = datetime(2023, 5, 4) #datetime(2015, 12, 1) is the first available date from Alpaca\n",
    "END_DATE = datetime(2023, 5, 8)\n",
    "#END_DATE = datetime(2023, 2, 4, hour=20)  # in ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it will take too much memory or take too long, we will send a request for every day between START_DATE end END_DATE. To get all trading days between these two days, we look at the trading days from SPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY_df = pd.read_csv(\n",
    "        f\"../data/alpaca/raw/d1/unadjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "SPY_df.set_index(SPY_df.index.tz_convert(\"US/Eastern\"), inplace=True) #Actually this isn't necessary, but we still do it for the sake of consistency\n",
    "SPY_df.set_index(SPY_df.index.tz_localize(None), inplace=True)\n",
    "dates = np.unique(pd.to_datetime(SPY_df.index).date)\n",
    "trading_dates = dates[(dates >= START_DATE.date()) & (dates <= END_DATE.date())]\n",
    "trading_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datetime.utcnow()} | Starting download\")\n",
    "with open(\"../data/alpaca/secret.txt\") as f:\n",
    "    PUBLIC_KEY = next(f).strip()\n",
    "    PRIVATE_KEY = next(f).strip()\n",
    "\n",
    "stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "\n",
    "for stock in SYMBOL_LIST:\n",
    "    os.makedirs(f\"../data/alpaca/raw/tick/{stock}\", exist_ok=True) #Create folder if it does not exist\n",
    "    for date in trading_dates:\n",
    "        start_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=4, minute=0)))\n",
    "        end_time = timezone(\"US/Eastern\").localize(datetime.combine(date, time(hour=20, minute=0)))\n",
    "        quote_request = StockQuotesRequest(\n",
    "            symbol_or_symbols=stock, start=start_time, end=end_time\n",
    "        )\n",
    "\n",
    "        quotes = stock_client.get_stock_quotes(quote_request)\n",
    "\n",
    "        quotes.df.to_csv(f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\")\n",
    "        print(f\"{datetime.utcnow()} | Downloaded {stock} tick data for {date.strftime('%Y-%m-%d')}\")\n",
    "        del quotes #Delete from memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Get tick data and convert to quotes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot just use resample and then ohlc, because of halts and other special trading conditions. Also we have to think about what OHLC means and what we want. To recap, the quotes contain the columns <code>[\"close_bid_size\", \"close_bid\", \"close_ask\", \"close_ask_size\", \"high_bid\", \"high_ask\", \"low_bid\", \"low_ask\", \"tradeable\"]</code>. \n",
    "\n",
    "For the corresponding bid or ask in a minute timestamp at 10:00:00:\n",
    "\n",
    "* close: we want the price at 10:00:59.99999..., which is the same as 10:01:00000... The quotes at this timestamp are the last quotes. Because if no new quotes come in, the last quotes stand. If the last available tick condition is not R ('regular trading'), the quote bar becomes untradable (and \"tradable\" = <code>False</code>). For example Y means a halt.\n",
    "* open: the open has no use, it is always equal to the previous close. That is why we do not have open prices in the quotes. It is **incorrect** to use ohlc() if we do want a opening bid/ask, because the open takes the first value in the bin. Which introduces look-ahead bias. It should use the last available value.\n",
    "* high: get highest value of regular trading (excluding non-regular trading)\n",
    "* low: get lowest value of regular trading (excluding non-regular trading)\n",
    "\n",
    "*Regular trading means no halts or weird conditions. It has nothing to do with pre- or post-market.*\n",
    "* Step 1: Resample closes\n",
    "* Step 2: Create a 'tradeable' column\n",
    "* Step 3: Get the high and low bid/asks\n",
    "* Step 4: Concatenate daily DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_LIST = [\"TOP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in SYMBOL_LIST:\n",
    "    quote_dfs = [] #A collection of dataframes for every day\n",
    "    for date in trading_dates:\n",
    "        tick_day_df = pd.read_csv(\n",
    "            f\"../data/alpaca/raw/tick/{stock}/{date.strftime('%Y-%m-%d')}.csv\",\n",
    "            usecols=[\n",
    "                \"timestamp\",\n",
    "                \"bid_size\",\n",
    "                \"bid_price\",\n",
    "                \"ask_price\",\n",
    "                \"ask_size\",\n",
    "                \"conditions\",\n",
    "            ],\n",
    "            index_col=\"timestamp\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        tick_day_df.index.names = [\"datetime\"]\n",
    "        \n",
    "        # Convert to ET-naive as always (raw data is always in UTC-aware)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_convert(\"US/Eastern\"), inplace=True)\n",
    "        tick_day_df.set_index(tick_day_df.index.tz_localize(None), inplace=True)\n",
    "\n",
    "        #Step 1: To get the close quotes and last trade condition for m1, resample and take last value\n",
    "        minute_df = tick_day_df.resample(\"1Min\").last()\n",
    "        minute_df.rename(\n",
    "            columns={\n",
    "                \"bid_size\": \"close_bid_size\",\n",
    "                \"bid_price\": \"close_bid\",\n",
    "                \"ask_price\": \"close_ask\",\n",
    "                \"ask_size\": \"close_ask_size\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        #Due to resampling it also creates rows for weekends and non-trading hours. Hence we need to shrink it\n",
    "        all_minutes_in_day = []\n",
    "        for hour in range(4, 20):\n",
    "            for minute in range(0, 60):\n",
    "                all_minutes_in_day.append(\n",
    "                    datetime.combine(date, time(hour=hour, minute=minute))\n",
    "                )\n",
    "        all_minutes_in_day = np.array(all_minutes_in_day)\n",
    "        assert len(all_minutes_in_day) == 16 * 60\n",
    "\n",
    "        minute_df = minute_df.reindex(all_minutes_in_day)\n",
    "\n",
    "        #Step 2: Create a 'tradeable' column which is False if there is no R in the conditions.\n",
    "        tradeable = np.vectorize(lambda condition_list: \"R\" in condition_list if condition_list is not None else False)\n",
    "\n",
    "        minute_df.ffill(inplace=True)\n",
    "        minute_df[\"tradeable\"] = tradeable(minute_df[\"conditions\"])\n",
    "        minute_df.drop(columns=[\"conditions\"], inplace=True)\n",
    "\n",
    "        #If there were no quotes for the first minutes of the day (very unlikely), we will forward fill them with the values of the previous day. We will do that later. 'tradeable' is already false in this case because there was no 'R' in the conditions.\n",
    "\n",
    "        #Step 3: Get the high and low bid/asks. If the value is empty, it means that during the entire minute there were no new quotes. Then we must use the last quotes. Since we already filled the close_bid and close_ask with the latest quote, we can set it equal to that.\n",
    "        high_df = tick_day_df.resample(\"1Min\").max()[[\"bid_price\", \"ask_price\"]]\n",
    "        high_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"high_bid\",\n",
    "                \"ask_price\": \"high_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        low_df = tick_day_df.resample(\"1Min\").min()[[\"bid_price\", \"ask_price\"]]\n",
    "        low_df.rename(\n",
    "            columns={\n",
    "                \"bid_price\": \"low_bid\",\n",
    "                \"ask_price\": \"low_ask\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=low_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "        minute_df = pd.merge(\n",
    "            left=minute_df, right=high_df, how=\"left\", left_index=True, right_index=True\n",
    "        )\n",
    "\n",
    "        # Fill high/low with close\n",
    "        minute_df[\"low_bid\"] = minute_df[\"low_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"low_ask\"] = minute_df[\"low_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "        minute_df[\"high_bid\"] = minute_df[\"high_bid\"].fillna(minute_df[\"close_bid\"])\n",
    "        minute_df[\"high_ask\"] = minute_df[\"high_ask\"].fillna(minute_df[\"close_ask\"])\n",
    "\n",
    "        # Reorder columns\n",
    "        minute_df = minute_df[\n",
    "                [\n",
    "                    \"close_bid_size\",\n",
    "                    \"close_bid\",\n",
    "                    \"close_ask\",\n",
    "                    \"close_ask_size\",\n",
    "                    \"high_bid\",\n",
    "                    \"high_ask\",\n",
    "                    \"low_bid\",\n",
    "                    \"low_ask\",\n",
    "                    \"tradeable\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        # Append the df (which only contains the minutes of 1 day) to quote_dfs.\n",
    "        quote_dfs.append(minute_df) #quote_dfs[0] contains the DataFrame from the first day; quote_dfs[1] the second day etc.\n",
    "\n",
    "    # Step 4: Concatenate daily DataFrames\n",
    "    quote_df = pd.concat(quote_dfs)\n",
    "\n",
    "    # As said in step 2, if there were no quotes for the first minutes of the day (very unlikely), we will forward fill them with the values of the previous day. 'tradeable' is already False. \n",
    "    if quote_df.isnull().any().any() == True:\n",
    "        amount_null = quote_df.isna().sum().sum()\n",
    "        print(\n",
    "            f\"{stock} | WARNING: dataframe contain {amount_null} null values, which will be forward filled.\"\n",
    "        )\n",
    "        quote_df.ffill(inplace=True)\n",
    "\n",
    "    quote_df.to_csv(f\"../data/alpaca/processed/m1/quotes/{stock}.csv\")\n",
    "    print(f\"{datetime.utcnow()} | {stock} | Processed tick data to quotes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
