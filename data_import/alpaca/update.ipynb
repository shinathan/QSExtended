{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data update\n",
    "\n",
    "If we want to update data, we should use append the data sets instead of downloading everything again. However if there are splits/dividends in the meantime, we should readjust all historical data. From Alpaca and most other vendors we can download adjusted and unadjusted daily data. So using that we calculate the adjustment factor. This is probably a crude way, but then I am sure that the adjustments are correct. I could also use the dividend/split endpoint, but that would overcomplicate things.\n",
    "\n",
    "For example lets assume that we have (already adjusted) data from day 1 to day 10. After close on day 15 we want to update the historical data. Lets assume that between and including day 11 to day 15 there was a 1 to 2 stock split. Then the day 10 adjustment factor will be 0.5x. Then all data from day 1 to day 10 should be multiplied by 0.5x. The data from day 11 to 15 is simply appended given that it is already adjusted. \n",
    "\n",
    "We will mostly just follow the same steps as in <code>bars.ipynb</code> and <code>tick.ipynb</code>, but instead of downloading everything we simply append. We also have to do some check to see if the dates make sense.\n",
    "\n",
    "* Step 1. Download and update all raw data.\n",
    "* Step 2. Using raw data, update processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "from alpaca.data.enums import Adjustment\n",
    "\n",
    "from datetime import datetime, time, timedelta\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_TO = datetime(2023, 7, 28) #ET time\n",
    "SYMBOL_LIST = [\"SPY\"]\n",
    "MARKET_HOURS_ONLY = True #If True, then the processed m1 data only contains market hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Update all raw data.**\n",
    "\n",
    "Raw data is everything in the <code>/raw</code> folder. These include 1-minute adjusted bars, 1-day adjusted and unadjusted bars, and tick data. \n",
    "\n",
    "Unadjusted 1-day bars and ticks can just be appended.\n",
    "For the 1-minute and 1-day unadjusted bars we can also just append them, however if there has been splits/dividends we must readjust all previous data.\n",
    "\n",
    "We only need to download the new data. For this we need to get a list of all trading dates. Just like before we will use SPY to determine this. Although this is a crude way. Because we may not have this data, we have to update SPY. It does not matter whether we use adjust or unadjusted, so I will just update the unadjusted because that is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be put in a function later. We should have a function for updating and for retrieving all trading dates and minutes.\n",
    "with open(\"../../data/alpaca/secret.txt\") as f:\n",
    "    PUBLIC_KEY = next(f).strip()\n",
    "    PRIVATE_KEY = next(f).strip()\n",
    "\n",
    "# Retrieve old daily data\n",
    "bars_old = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/unadjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "\n",
    "last_available_date = bars_old.index[-1].tz_localize(None)\n",
    "first_day_of_update = last_available_date + timedelta(days=1) # Day+1 may not be a trading day, but since Alpaca does not return non-trading days, this will work\n",
    "\n",
    "# Get new bars\n",
    "if first_day_of_update.date() > UPDATE_TO.date():\n",
    "    print(\"We already have enough data for SPY\")\n",
    "    #continue\n",
    "else:\n",
    "    stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "    spy_request = StockBarsRequest(\n",
    "        symbol_or_symbols=\"SPY\",\n",
    "        # If we do not replace hour with 0, we do not get the first day.\n",
    "        start=timezone(\"US/Eastern\").localize(first_day_of_update.replace(hour=0)),\n",
    "        end=timezone(\"US/Eastern\").localize(UPDATE_TO.replace(hour=0)),\n",
    "        timeframe=TimeFrame(1, TimeFrameUnit.Day),\n",
    "        adjustment=Adjustment.RAW,\n",
    "    )\n",
    "    bars_new = stock_client.get_stock_bars(spy_request).df\n",
    "\n",
    "    bars_new = bars_new.loc[\"SPY\"][[\"close\", \"volume\"]]\n",
    "    bars_new.index.names = [\"datetime\"]\n",
    "\n",
    "# Combine all\n",
    "bars_all = pd.concat([bars_old, bars_new])\n",
    "bars_all.to_csv(f\"../../data/alpaca/raw/d1/unadjusted/SPY.csv\")\n",
    "print(f\"Updated SPY d1 unadjusted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all SPY data\n",
    "SPY_df = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/unadjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "SPY_df.set_index(SPY_df.index.tz_localize(None), inplace=True)\n",
    "\n",
    "# Get a list of all trading days\n",
    "all_trading_dates = pd.to_datetime(SPY_df.index).date\n",
    "\n",
    "# Get a list of all trading minutes (note: all last minute are hence 19:59)\n",
    "all_trading_minutes = []\n",
    "amount_of_days = len(all_trading_dates)\n",
    "for date in all_trading_dates:\n",
    "    for hour in range(4, 20):\n",
    "        for minute in range(0, 60):\n",
    "            all_trading_minutes.append(datetime.combine(date, time(hour=hour, minute=minute)))\n",
    "all_trading_minutes = np.array(all_trading_minutes)\n",
    "\n",
    "assert len(all_trading_minutes) == amount_of_days * 16 * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally download our data and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = \"O\"\n",
    "\n",
    "# UNADJUSTED DAILY BARS\n",
    "# Retrieve old daily data\n",
    "bars_old = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/unadjusted/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "\n",
    "last_available_date = bars_old.index[-1].tz_localize(None)\n",
    "first_day_of_update = last_available_date + timedelta(days=1) # Day+1 may not be a trading day, but since Alpaca does not return non-trading days, this will work\n",
    "\n",
    "# Get new bars\n",
    "if first_day_of_update.date() > UPDATE_TO.date():\n",
    "    print(f\"We already have enough data for {stock}\")\n",
    "else:\n",
    "    stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "    stock_request = StockBarsRequest(\n",
    "        symbol_or_symbols=stock,\n",
    "        # If we do not replace hour with 0, we do not get the first day.\n",
    "        start=timezone(\"US/Eastern\").localize(first_day_of_update.replace(hour=0)),\n",
    "        end=timezone(\"US/Eastern\").localize(UPDATE_TO.replace(hour=0)),\n",
    "        timeframe=TimeFrame(1, TimeFrameUnit.Day),\n",
    "        adjustment=Adjustment.RAW,\n",
    "    )\n",
    "    bars_new = stock_client.get_stock_bars(stock_request).df\n",
    "\n",
    "    bars_new = bars_new.loc[stock][[\"close\", \"volume\"]]\n",
    "    bars_new.index.names = [\"datetime\"]\n",
    "\n",
    "    # Combine all\n",
    "    bars_all = pd.concat([bars_old, bars_new])\n",
    "    bars_all.to_csv(f\"../../data/alpaca/raw/d1/unadjusted/{stock}.csv\")\n",
    "\n",
    "    print(f\"{datetime.utcnow().replace(microsecond=0)} | Updated {stock} d1 unadjusted from {first_day_of_update.strftime('%Y-%m-%d')} to {UPDATE_TO.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# ADJUSTED DAILY BARS\n",
    "# Retrieve old daily data\n",
    "bars_old = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/adjusted/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "\n",
    "last_available_date = bars_old.index[-1].tz_localize(None)\n",
    "first_day_of_update = last_available_date + timedelta(days=1) # Day+1 may not be a trading day, but since Alpaca does not return non-trading days, this will work\n",
    "\n",
    "# Get new bars\n",
    "if first_day_of_update.date() > UPDATE_TO.date():\n",
    "    print(f\"We already have enough data for {stock}\")\n",
    "    #continue\n",
    "else:\n",
    "    stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "    \"\"\"\n",
    "    WARNING: because we have to calculate the adjustment factor we also need the data at last_available_date\n",
    "    If we have data from day 1 to day 10 which we need rejust, we need the NEW adjustment factor at day 10, not day 11.\n",
    "    But when combining we need to leave out last_available_date, else there will be two entries with last_available_date\n",
    "    \"\"\"\n",
    "    stock_request = StockBarsRequest(\n",
    "        symbol_or_symbols=stock,\n",
    "        start=timezone(\"US/Eastern\").localize(first_day_of_update.replace(hour=0) - timedelta(days=1)),\n",
    "        end=timezone(\"US/Eastern\").localize(UPDATE_TO.replace(hour=0)),\n",
    "        timeframe=TimeFrame(1, TimeFrameUnit.Day),\n",
    "        adjustment=Adjustment.ALL,\n",
    "    )\n",
    "    bars_new = stock_client.get_stock_bars(stock_request).df\n",
    "\n",
    "    bars_new = bars_new.loc[stock][[\"close\", \"volume\"]]\n",
    "    bars_new.index.names = [\"datetime\"]\n",
    "\n",
    "    # Readjust bars_old using adjustment factor\n",
    "        # Calculate adjustment factor\n",
    "    last_close_old = bars_old.iloc[-1]\n",
    "    last_close_new = bars_new.iloc[0]\n",
    "\n",
    "    # If old is 50, new is 100, old should be X2\n",
    "    adjustment_factor = last_close_new / last_close_old\n",
    "    bars_old['close'] = bars_old['close'] * adjustment_factor['close']\n",
    "    bars_old['volume'] = bars_old['volume'] / adjustment_factor['close'] #If stock split X2, volume should be X0.5\n",
    "\n",
    "    # Strip first bar of bars_new, because we already have it\n",
    "    bars_new = bars_new[1:]\n",
    "\n",
    "    # Combine all\n",
    "    bars_all = pd.concat([bars_old, bars_new])\n",
    "    bars_all.to_csv(f\"../../data/alpaca/raw/d1/adjusted/{stock}.csv\")\n",
    "\n",
    "    print(f\"{datetime.utcnow().replace(microsecond=0)} | Updated {stock} d1 adjusted from {first_day_of_update.strftime('%Y-%m-%d')} to {UPDATE_TO.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# ADJUSTED MINUTE BARS\n",
    "# Retrieve old minute data\n",
    "bars_old = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/m1/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "\n",
    "last_available_date = bars_old.index[-1].tz_localize(None)\n",
    "first_day_of_update = last_available_date + timedelta(days=1) # Day+1 may not be a trading day, but since Alpaca does not return non-trading days, this will work\n",
    "\n",
    "# Get new bars\n",
    "if first_day_of_update.date() > UPDATE_TO.date():\n",
    "    print(f\"We already have enough data for {stock}\")\n",
    "    #continue\n",
    "else:\n",
    "    stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "    stock_request = StockBarsRequest(\n",
    "        symbol_or_symbols=stock,\n",
    "        start=timezone(\"US/Eastern\").localize(first_day_of_update.replace(hour=4) - timedelta(days=1)),\n",
    "        end=timezone(\"US/Eastern\").localize(UPDATE_TO.replace(hour=20)),\n",
    "        timeframe=TimeFrame(1, TimeFrameUnit.Minute),\n",
    "        adjustment=Adjustment.ALL,\n",
    "    )\n",
    "    bars_new = stock_client.get_stock_bars(stock_request).df\n",
    "\n",
    "    bars_new = bars_new.loc[stock][[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "    bars_new.index.names = [\"datetime\"]\n",
    "\n",
    "    # Get adjustment factor from daily data\n",
    "    stock_df_unadjusted = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/unadjusted/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    stock_df_adjusted = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/adjusted/{stock}.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    if not stock_df_adjusted.index.equals(stock_df_unadjusted.index):\n",
    "        raise Exception(\n",
    "            \"The indices in the adjusted and unadjusted DataFrames are not equal.\"\n",
    "        )\n",
    "    adjustment = stock_df_adjusted / stock_df_unadjusted\n",
    "    adjustment.index = adjustment.index.date\n",
    "    adjustment.rename(columns={\"close\": \"adjustment\"}, inplace=True)\n",
    "\n",
    "    # Get difference in adjustment factors\n",
    "    adjustment_factor_last_day = adjustment[adjustment.index == last_available_date.date()]['adjustment']\n",
    "    adjustment_factor_last_day_plus_one = adjustment.iloc[adjustment.index.get_loc(adjustment_factor_last_day.index[0]) + 1]['adjustment']\n",
    "\n",
    "    difference_adjustment = adjustment_factor_last_day_plus_one / adjustment_factor_last_day\n",
    "    difference_adjustment = difference_adjustment[0]\n",
    "\n",
    "    bars_old['open'] = bars_old['open'] * difference_adjustment\n",
    "    bars_old['high'] = bars_old['high'] * difference_adjustment\n",
    "    bars_old['low'] = bars_old['low'] * difference_adjustment\n",
    "    bars_old['close'] = bars_old['close'] * difference_adjustment\n",
    "    bars_old['volume'] = bars_old['volume'] / difference_adjustment\n",
    "\n",
    "    # Combine all\n",
    "    bars_all = pd.concat([bars_old, bars_new])\n",
    "    bars_all.to_csv(f\"../../data/alpaca/raw/m1/{stock}.csv\")\n",
    "\n",
    "    print(f\"{datetime.utcnow().replace(microsecond=0)} | Updated {stock} m1 adjusted from {first_day_of_update.strftime('%Y-%m-%d')} to {UPDATE_TO.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # TICK DATA\n",
    "    # Simply rerun step 1 in tick.ipynb, but only download the required data by changing the start and end dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Using raw data, update processed data**\n",
    "\n",
    "Now the data processing steps 2 and 4 in <code>bars.ipynb</code> can be run again. Those steps only use raw data, which is now updated and correct. It's fine to run it all, because we already have all data. So that won't take long.\n",
    "\n",
    "For quotes and quotebars (step 2 to 4) we can also do this, but it will take longer. But still, since all data is already downloaded, this is not a problem. The data download is the most time-consuming.\n",
    "\n",
    "I could make a script to only update the processed data, but that would be extra code and potential bugs for no real benefit. Also then we need to again take into account readjustments, which are very annoying.\n",
    "\n",
    "In live trading, the assets that are traded should be recorded. For these we should not update using historical data. Because then we can properly compare 'backtest' results and real results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
