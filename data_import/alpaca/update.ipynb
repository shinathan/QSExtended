{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data update\n",
    "\n",
    "If we want to update data, we should use append the data sets instead of downloading everything again. However if there are splits/dividends in the meantime, we should readjust all historical data. From Alpaca and most other vendors we can download adjusted and unadjusted daily data. So using that we calculate the adjustment factor. This is probably a crude way, but then I am sure that the adjustments are correct. I could also use the dividend/split endpoint, but that would overcomplicate things.\n",
    "\n",
    "For example lets assume that we have (already adjusted) data from day 1 to day 10. After close on day 15 we want to update the historical data. Lets assume that between and including day 11 to day 15 there was a 1 to 2 stock split. Then the day 10 adjustment factor will be 0.5x. Then all data from day 1 to day 10 should be multiplied by 0.5x. The data from day 11 to 15 is simply appended given that it is already adjusted. \n",
    "\n",
    "We will mostly just follow the same steps as in <code>bars.ipynb</code> and <code>tick.ipynb</code>, but instead of downloading everything we simply append. We also have to do some check to see if the dates make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "from alpaca.data.enums import Adjustment\n",
    "\n",
    "from datetime import datetime, time, timedelta\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_TO = datetime(2023, 7, 28) #ET time\n",
    "SYMBOL_LIST = [\"SPY\"]\n",
    "MARKET_HOURS_ONLY = True #If True, then the processed m1 data only contains market hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Download m1 data and append.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded SPY adjusted\n",
      "Downloaded SPY unadjusted\n"
     ]
    }
   ],
   "source": [
    "# First get a list of trading dates using SPY. We will download all available SPY daily data in case we do not have it.\n",
    "with open(\"../../data/alpaca/secret.txt\") as f:\n",
    "    PUBLIC_KEY = next(f).strip()\n",
    "    PRIVATE_KEY = next(f).strip()\n",
    "\n",
    "stock_client = StockHistoricalDataClient(PUBLIC_KEY, PRIVATE_KEY)\n",
    "spy_request = StockBarsRequest(\n",
    "    symbol_or_symbols=\"SPY\",\n",
    "    start=timezone(\"US/Eastern\").localize(datetime(2015, 12, 1)),\n",
    "    end=timezone(\"US/Eastern\").localize(UPDATE_TO),\n",
    "    timeframe=TimeFrame(1, TimeFrameUnit.Day),\n",
    "    adjustment=Adjustment.RAW,\n",
    ")\n",
    "bars = stock_client.get_stock_bars(spy_request).df\n",
    "\n",
    "spy_df = bars.loc[\"SPY\"][[\"close\", \"volume\"]]\n",
    "spy_df.index.names = [\"datetime\"]\n",
    "spy_df.to_csv(f\"../../data/alpaca/raw/d1/unadjusted/SPY.csv\")\n",
    "print(f\"Downloaded SPY adjusted\")\n",
    "\n",
    "spy_request = StockBarsRequest(\n",
    "    symbol_or_symbols=\"SPY\",\n",
    "    start=timezone(\"US/Eastern\").localize(datetime(2015, 12, 1)),\n",
    "    end=timezone(\"US/Eastern\").localize(UPDATE_TO),\n",
    "    timeframe=TimeFrame(1, TimeFrameUnit.Day),\n",
    "    adjustment=Adjustment.ALL,\n",
    ")\n",
    "bars = stock_client.get_stock_bars(spy_request).df\n",
    "\n",
    "spy_df = bars.loc[\"SPY\"][[\"close\"]]\n",
    "spy_df.index.names = [\"datetime\"]\n",
    "spy_df.to_csv(f\"../../data/alpaca/raw/d1/adjusted/SPY.csv\")\n",
    "print(f\"Downloaded SPY unadjusted\")\n",
    "\n",
    "# Retrieve downloaded data\n",
    "SPY_df = pd.read_csv(\n",
    "        f\"../../data/alpaca/raw/d1/adjusted/SPY.csv\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "SPY_df.set_index(SPY_df.index.tz_localize(None), inplace=True)\n",
    "all_trading_dates = pd.to_datetime(SPY_df.index).date\n",
    "\n",
    "# Also get a list of all minutes (note: all last minute are hence 19:59)\n",
    "all_minutes = []\n",
    "amount_of_days = len(all_trading_dates)\n",
    "for date in all_trading_dates:\n",
    "    for hour in range(4, 20):\n",
    "        for minute in range(0, 60):\n",
    "            all_minutes.append(datetime.combine(date, time(hour=hour, minute=minute)))\n",
    "all_minutes = np.array(all_minutes)\n",
    "\n",
    "assert len(all_minutes) == amount_of_days * 16 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-31 13:24:49 | Download AMC m1 raw data from 2023-07-24 to 2023-07-28\n",
      "2023-07-31 13:24:50 | Updated AMC m1 raw data\n"
     ]
    }
   ],
   "source": [
    "stock = \"AMC\"\n",
    "\n",
    "# We want to append only the new bars. So we need the start date of the update. \n",
    "bars_old = pd.read_csv(\n",
    "    f\"../../data/alpaca/raw/m1/{stock}.csv\",\n",
    "    index_col=\"datetime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "last_available_date = bars_old.index[-1].date()\n",
    "trading_dates_to_update = all_trading_dates[(all_trading_dates >= last_available_date + timedelta(days=1)) & (all_trading_dates <= UPDATE_TO.date())]\n",
    "\n",
    "# Get new bars\n",
    "stock_request = StockBarsRequest(\n",
    "    symbol_or_symbols=stock,\n",
    "    start=timezone(\"US/Eastern\").localize(datetime.combine(trading_dates_to_update[0], time(hour=4))),\n",
    "    end=timezone(\"US/Eastern\").localize(datetime.combine(trading_dates_to_update[-1], time(hour=20))),\n",
    "    timeframe=TimeFrame(1, TimeFrameUnit.Minute),\n",
    "    adjustment=Adjustment.ALL,\n",
    ")\n",
    "print(f\"{datetime.utcnow().replace(microsecond=0)} | Download {stock} m1 raw data from {trading_dates_to_update[0].strftime('%Y-%m-%d')} to {trading_dates_to_update[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "bars_new = stock_client.get_stock_bars(stock_request).df\n",
    "\n",
    "bars_new = bars_new.loc[stock][[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "bars_new.index.names = [\"datetime\"]\n",
    "\n",
    "# Combine old and new bars\n",
    "bars_all = pd.concat([bars_old, bars_new])\n",
    "\n",
    "if len(bars_all.index[bars_all.index.duplicated()]) != 0:\n",
    "    raise Exception('When merging old and new bars, there were duplicate dates.')\n",
    "\n",
    "#bars_all.to_csv(f\"../../data/alpaca/raw/m1/{stock}.csv\")\n",
    "print(f\"{datetime.utcnow().replace(microsecond=0)} | Updated {stock} m1 raw data\")\n",
    "\n",
    "# 2. UPDATE PROCESSED BARS\n",
    "# Processed bars always has the same as raw bars because in bars.ipynb we processed all raw data.\n",
    "# So we do not have to worry about dates not being equal\n",
    "bars_processed_new = bars_new.copy() \n",
    "bars_processed_new[\"tradeable\"] = True\n",
    "\n",
    "bars_processed_new.set_index(bars_processed_new.index.tz_convert(\"US/Eastern\"), inplace=True)\n",
    "bars_processed_new.set_index(bars_processed_new.index.tz_localize(None), inplace=True)\n",
    "\n",
    "start_datetime = bars_processed_new.index[0]\n",
    "end_datetime = bars_processed_new.index[-1]\n",
    "\n",
    "stock_minutes = all_minutes[\n",
    "    (all_minutes >= start_datetime.replace(hour=4, minute=0, second=0))\n",
    "    & (all_minutes <= end_datetime.replace(hour=19, minute=59, second=0))\n",
    "]\n",
    "\n",
    "bars_processed_new = bars_processed_new.reindex(stock_minutes)\n",
    "\n",
    "# Get old processed bars\n",
    "bars_processed_old = pd.read_csv(\n",
    "    f\"../../data/alpaca/processed/m1/bars/{stock}.csv\",\n",
    "    index_col=\"datetime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "# Combine old + new\n",
    "bars_processed_all = pd.concat([bars_processed_old, bars_processed_new])\n",
    "\n",
    "# Fill empty values with last available price, which is the last close.\n",
    "bars_processed_all[\"tradeable\"].fillna(False, inplace=True)\n",
    "bars_processed_all[\"volume\"].fillna(0, inplace=True)\n",
    "bars_processed_all[\"close\"] = bars_processed_all[\"close\"].fillna(method=\"ffill\")\n",
    "\n",
    "bars_processed_all[\"open\"] = bars_processed_all[\"open\"].fillna(bars_processed_all[\"close\"])\n",
    "bars_processed_all[\"low\"] = bars_processed_all[\"low\"].fillna(bars_processed_all[\"close\"])\n",
    "bars_processed_all[\"high\"] = bars_processed_all[\"high\"].fillna(bars_processed_all[\"close\"])\n",
    "\n",
    "# In contrast to the initial download, we do not have to backfill. That is also why we\n",
    "# forward filled after combining. If we did it before, there could still be empty values\n",
    "# for the first few values of the updated bars. Backfilling them would be wrong because\n",
    "# we have old data to forward fill with.\n",
    "\n",
    "if MARKET_HOURS_ONLY == True:\n",
    "    bars_processed_all = bars_processed_all.between_time(\"9:30\", \"15:59\")\n",
    "\n",
    "if len(bars_processed_all.index[bars_processed_all.index.duplicated()]) != 0:\n",
    "    raise Exception('When merging old and new bars, there were duplicate dates.')\n",
    "\n",
    "bars_processed_all.to_csv(f\"../../data/alpaca/processed/m1/bars/{stock}.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
